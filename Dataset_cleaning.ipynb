{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "abbe9b4d-38d0-4112-8ab4-caf0ee11de43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 records:\n",
      "       name   id nametype     recclass  mass (g)  fall    year    reclat  \\\n",
      "0    Aachen    1    Valid           L5      21.0  Fell  1880.0  50.77500   \n",
      "1    Aarhus    2    Valid           H6     720.0  Fell  1951.0  56.18333   \n",
      "2      Abee    6    Valid          EH4  107000.0  Fell  1952.0  54.21667   \n",
      "3  Acapulco   10    Valid  Acapulcoite    1914.0  Fell  1976.0  16.88333   \n",
      "4   Achiras  370    Valid           L6     780.0  Fell  1902.0 -33.16667   \n",
      "\n",
      "     reclong           GeoLocation  Unnamed: 10  \n",
      "0    6.08333     (50.775, 6.08333)          NaN  \n",
      "1   10.23333  (56.18333, 10.23333)          NaN  \n",
      "2 -113.00000    (54.21667, -113.0)          NaN  \n",
      "3  -99.90000     (16.88333, -99.9)          NaN  \n",
      "4  -64.95000   (-33.16667, -64.95)          NaN  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file (ensure the file is in the same directory as your notebook)\n",
    "df = pd.read_csv(\"Meteorite_Landings.csv\")\n",
    "\n",
    "# Display the first 5 records\n",
    "print(\"First 5 records:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "705e20ce-7afc-4fd7-a168-7e287e2c9087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of duplicate records based on 'id': 0\n",
      "\n",
      "Missing values per column:\n",
      "name               0\n",
      "id                 0\n",
      "nametype           0\n",
      "recclass           0\n",
      "mass (g)         131\n",
      "fall               0\n",
      "year             291\n",
      "reclat          7315\n",
      "reclong         7315\n",
      "GeoLocation     7315\n",
      "Unnamed: 10    45716\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates based on the 'id' column\n",
    "duplicate_count = df.duplicated(subset=[\"id\"]).sum()\n",
    "print(f\"\\nNumber of duplicate records based on 'id': {duplicate_count}\")\n",
    "\n",
    "# Drop duplicates if necessary\n",
    "df = df.drop_duplicates(subset=[\"id\"])\n",
    "\n",
    "# Check for missing values in each column\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "598fea7b-79e7-4806-b5f0-aa3f80db6dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows left after dropping rows with missing values: 38115\n",
      "\n",
      "Updated DataFrame info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 38115 entries, 0 to 45715\n",
      "Data columns (total 10 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   name         38115 non-null  object \n",
      " 1   id           38115 non-null  int64  \n",
      " 2   nametype     38115 non-null  object \n",
      " 3   recclass     38115 non-null  object \n",
      " 4   mass (g)     38115 non-null  float64\n",
      " 5   fall         38115 non-null  object \n",
      " 6   year         38115 non-null  float64\n",
      " 7   reclat       38115 non-null  float64\n",
      " 8   reclong      38115 non-null  float64\n",
      " 9   GeoLocation  38115 non-null  object \n",
      "dtypes: float64(4), int64(1), object(5)\n",
      "memory usage: 3.2+ MB\n",
      "None\n",
      "\n",
      "First 5 rows of the updated DataFrame:\n",
      "       name   id nametype     recclass  mass (g)  fall    year    reclat  \\\n",
      "0    Aachen    1    Valid           L5      21.0  Fell  1880.0  50.77500   \n",
      "1    Aarhus    2    Valid           H6     720.0  Fell  1951.0  56.18333   \n",
      "2      Abee    6    Valid          EH4  107000.0  Fell  1952.0  54.21667   \n",
      "3  Acapulco   10    Valid  Acapulcoite    1914.0  Fell  1976.0  16.88333   \n",
      "4   Achiras  370    Valid           L6     780.0  Fell  1902.0 -33.16667   \n",
      "\n",
      "     reclong           GeoLocation  \n",
      "0    6.08333     (50.775, 6.08333)  \n",
      "1   10.23333  (56.18333, 10.23333)  \n",
      "2 -113.00000    (54.21667, -113.0)  \n",
      "3  -99.90000     (16.88333, -99.9)  \n",
      "4  -64.95000   (-33.16667, -64.95)  \n"
     ]
    }
   ],
   "source": [
    "df.drop(columns=[\"Unnamed: 10\", \"unknown\"], inplace=True, errors='ignore')\n",
    "\n",
    "# 2. Drop all rows that have *any* missing values in *any* column\n",
    "df.dropna(how='any', inplace=True)\n",
    "\n",
    "# 3. Check how many rows remain\n",
    "print(f\"Number of rows left after dropping rows with missing values: {len(df)}\")\n",
    "\n",
    "# Optional: Display the updated DataFrame structure\n",
    "print(\"\\nUpdated DataFrame info:\")\n",
    "print(df.info())\n",
    "print(\"\\nFirst 5 rows of the updated DataFrame:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "f2c1aba9-0fd1-441b-acda-82c0858751ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Types Before:\n",
      "name            object\n",
      "id               int64\n",
      "nametype        object\n",
      "recclass        object\n",
      "mass (g)       float64\n",
      "fall            object\n",
      "year           float64\n",
      "reclat         float64\n",
      "reclong        float64\n",
      "GeoLocation     object\n",
      "dtype: object\n",
      "\n",
      "Data Types After:\n",
      "name            object\n",
      "id               int64\n",
      "nametype        object\n",
      "recclass        object\n",
      "mass (g)       float64\n",
      "fall            object\n",
      "year             int64\n",
      "reclat         float64\n",
      "reclong        float64\n",
      "GeoLocation     object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# We assume df is already loaded and missing values dropped\n",
    "print(\"Data Types Before:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# 1. Convert 'id' to int if it’s truly an integer ID\n",
    "df[\"id\"] = df[\"id\"].astype(int)\n",
    "\n",
    "# 2. Convert 'year' to integer if it’s just a year (e.g., 1990)\n",
    "#    or to datetime if it’s a full date. Let's assume it's just a year:\n",
    "df[\"year\"] = df[\"year\"].astype(int)\n",
    "\n",
    "print(\"\\nData Types After:\")\n",
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "3f31bdb2-2e5a-45cb-bde8-624310bc4e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_mass = df[df[\"mass (g)\"] < 0]\n",
    "if not invalid_mass.empty:\n",
    "    print(\"\\nRows with negative mass:\")\n",
    "    print(invalid_mass)\n",
    "    # Optionally drop or fix these rows\n",
    "    # df = df[df[\"mass (g)\"] >= 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "be921282-d021-464d-b82c-c2bd5276034e",
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_mass = df[df[\"mass (g)\"] < 0]\n",
    "if not invalid_mass.empty:\n",
    "    print(\"\\nRows with negative mass:\")\n",
    "    print(invalid_mass)\n",
    "    # Optionally drop or fix these rows\n",
    "    # df = df[df[\"mass (g)\"] >= 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "ed8ff43f-b893-411f-acb6-49b973130d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_lat = df[(df[\"reclat\"] < -90) | (df[\"reclat\"] > 90)]\n",
    "invalid_lon = df[(df[\"reclong\"] < -180) | (df[\"reclong\"] > 180)]\n",
    "\n",
    "if not invalid_lat.empty or not invalid_lon.empty:\n",
    "    print(\"\\nInvalid lat/long rows:\")\n",
    "    print(pd.concat([invalid_lat, invalid_lon]).drop_duplicates())\n",
    "    # Optionally drop them or fix them:\n",
    "    # df = df[(df[\"reclat\"] >= -90) & (df[\"reclat\"] <= 90) &\n",
    "    #         (df[\"reclong\"] >= -180) & (df[\"reclong\"] <= 180)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "69086eb4-0718-4be4-990d-fec1bc96438b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rows with invalid years:\n",
      "                        name     id nametype recclass  mass (g)   fall  year  \\\n",
      "30682  Northwest Africa 7701  57150    Valid      CK6      55.0  Found  2101   \n",
      "\n",
      "       reclat  reclong GeoLocation  \n",
      "30682     0.0      0.0  (0.0, 0.0)  \n"
     ]
    }
   ],
   "source": [
    "# Example: Keep only years between 800 and 2025\n",
    "invalid_years = df[(df[\"year\"] < 800) | (df[\"year\"] > 2025)]\n",
    "if not invalid_years.empty:\n",
    "    print(\"\\nRows with invalid years:\")\n",
    "    print(invalid_years)\n",
    "    # Optionally drop them:\n",
    "    # df = df[(df[\"year\"] >= 800) & (df[\"year\"] <= 2025)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "68cfd80f-94d7-4c39-b5a7-ecb33cb40ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique values in 'fall':\n",
      "['Fell' 'Found']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nUnique values in 'fall':\")\n",
    "print(df[\"fall\"].unique())\n",
    "\n",
    "# If you see weird typos or extra spaces, you can standardize:\n",
    "df[\"fall\"] = df[\"fall\"].str.strip().str.title()  # e.g. \"Fell\", \"Found\"\n",
    "valid_fall_values = [\"Fell\", \"Found\"]\n",
    "\n",
    "invalid_fall = df[~df[\"fall\"].isin(valid_fall_values)]\n",
    "if not invalid_fall.empty:\n",
    "    print(\"\\nInvalid 'fall' values found:\")\n",
    "    print(invalid_fall[\"fall\"].value_counts())\n",
    "    # Optionally drop or correct them\n",
    "    # df = df[df[\"fall\"].isin(valid_fall_values)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0f94fc-0f71-45c1-9d4c-60d497edfc9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "76be880a-a19a-4eec-9b5f-e8aea4c96ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Some unique recclass values:\n",
      "['L5' 'H6' 'EH4' 'Acapulcoite' 'L6' 'LL3-6' 'H5' 'L' 'Diogenite-pm' 'H4'\n",
      " 'H' 'Iron, IVA' 'CR2-an' 'LL5' 'CI1' 'L/LL4' 'Eucrite-mmict' 'CV3'\n",
      " 'Ureilite-an' 'Stone-uncl']\n"
     ]
    }
   ],
   "source": [
    "df[\"recclass\"] = df[\"recclass\"].str.strip()\n",
    "\n",
    "print(\"\\nSome unique recclass values:\")\n",
    "print(df[\"recclass\"].unique()[:20])  # Show first 20 unique classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "7ea6f721-1928-43ed-a9b2-0aae93ff441c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows remaining after dropping invalid years: 38114\n"
     ]
    }
   ],
   "source": [
    "# Drop all rows outside the 800–2025 range\n",
    "df = df[(df[\"year\"] >= 800) & (df[\"year\"] <= 2025)]\n",
    "\n",
    "print(\"Rows remaining after dropping invalid years:\", len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "4d56a214-8916-45d0-829a-aca519d86b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of invalid years after clipping: 0\n"
     ]
    }
   ],
   "source": [
    "# Clip the 'year' values between 800 and 2025\n",
    "df[\"year\"] = df[\"year\"].clip(lower=800, upper=2025)\n",
    "\n",
    "# Verify if clipping worked\n",
    "invalid_years = df[(df[\"year\"] < 800) | (df[\"year\"] > 2025)]\n",
    "print(\"Number of invalid years after clipping:\", len(invalid_years))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "d067aec5-94e2-46f0-be17-6eb32f655a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       (50.775, 6.08333)\n",
      "1    (56.18333, 10.23333)\n",
      "2      (54.21667, -113.0)\n",
      "3       (16.88333, -99.9)\n",
      "4     (-33.16667, -64.95)\n",
      "5            (32.1, 71.8)\n",
      "6    (44.83333, 95.16667)\n",
      "7     (44.21667, 0.61667)\n",
      "8      (-31.6, -65.23333)\n",
      "9     (-30.86667, -64.55)\n",
      "Name: GeoLocation, dtype: object\n",
      "\n",
      "Number of unique GeoLocation values: 16907\n",
      "\n",
      "Number of nulls in GeoLocation: 0\n"
     ]
    }
   ],
   "source": [
    "# Check the first few values of GeoLocation\n",
    "print(df[\"GeoLocation\"].head(10))\n",
    "\n",
    "# Check how many unique values\n",
    "print(\"\\nNumber of unique GeoLocation values:\", df[\"GeoLocation\"].nunique())\n",
    "\n",
    "# Check for nulls\n",
    "print(\"\\nNumber of nulls in GeoLocation:\", df[\"GeoLocation\"].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "ef4b28e2-8f6d-42ff-a2b4-0c29f6209635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 38114 entries, 0 to 45715\n",
      "Data columns (total 10 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   name         38114 non-null  object \n",
      " 1   id           38114 non-null  int64  \n",
      " 2   nametype     38114 non-null  object \n",
      " 3   recclass     38114 non-null  object \n",
      " 4   mass (g)     38114 non-null  float64\n",
      " 5   fall         38114 non-null  object \n",
      " 6   year         38114 non-null  int64  \n",
      " 7   reclat       38114 non-null  float64\n",
      " 8   reclong      38114 non-null  float64\n",
      " 9   GeoLocation  38114 non-null  object \n",
      "dtypes: float64(3), int64(2), object(5)\n",
      "memory usage: 3.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "e19b91a1-83bd-4f38-a623-b6e7ade0d97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38114\n"
     ]
    }
   ],
   "source": [
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "a7be535a-6eaa-403e-b383-bb8f97a4be5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, 'id' is unique for every row in the dataset.\n"
     ]
    }
   ],
   "source": [
    "num_rows = len(df)\n",
    "num_unique_ids = df[\"id\"].nunique()\n",
    "\n",
    "if num_unique_ids == num_rows:\n",
    "    print(\"Yes, 'id' is unique for every row in the dataset.\")\n",
    "else:\n",
    "    print(f\"No, 'id' is not unique. Unique IDs: {num_unique_ids}, Total Rows: {num_rows}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "6d4a1fd5-72a2-41da-b41f-2e4453aded23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows after sorting by 'id':\n",
      "        name  id nametype recclass  mass (g)   fall  year    reclat  \\\n",
      "0     Aachen   1    Valid       L5      21.0   Fell  1880  50.77500   \n",
      "1     Aarhus   2    Valid       H6     720.0   Fell  1951  56.18333   \n",
      "1111   Abajo   4    Valid       H5     331.0  Found  1982  26.80000   \n",
      "1113  Abbott   5    Valid     H3-6   21100.0  Found  1951  36.30000   \n",
      "2       Abee   6    Valid      EH4  107000.0   Fell  1952  54.21667   \n",
      "\n",
      "        reclong           GeoLocation  \n",
      "0       6.08333     (50.775, 6.08333)  \n",
      "1      10.23333  (56.18333, 10.23333)  \n",
      "1111 -105.41667    (26.8, -105.41667)  \n",
      "1113 -104.28333    (36.3, -104.28333)  \n",
      "2    -113.00000    (54.21667, -113.0)  \n"
     ]
    }
   ],
   "source": [
    "# Sort the DataFrame by 'id' in increasing order\n",
    "df.sort_values(by='id', inplace=True)\n",
    "\n",
    "# Display the first 5 rows to verify\n",
    "print(\"First 5 rows after sorting by 'id':\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "ac3c9899-edc5-4622-88d6-54e5c6261ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns after renaming:\n",
      "Index(['meteorite_name', 'meteorite_id', 'nametype', 'class', 'mass',\n",
      "       'fall_type', 'fall_year', 'latitude', 'longitude', 'GeoLocation'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Assuming your DataFrame is called df\n",
    "df.rename(columns={\n",
    "    \"id\": \"meteorite_id\",\n",
    "    \"name\": \"meteorite_name\",\n",
    "    \"recclass\": \"class\",\n",
    "    \"mass (g)\": \"mass\",\n",
    "    \"fall\": \"fall_type\",\n",
    "    \"year\": \"fall_year\",\n",
    "    \"reclat\": \"latitude\",\n",
    "    \"reclong\": \"longitude\"\n",
    "}, inplace=True)\n",
    "\n",
    "# Verify the new column names\n",
    "print(\"Columns after renaming:\")\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "592d5677-c861-4e16-8fe3-16bda4c7e0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     meteorite_name  meteorite_id nametype class      mass fall_type  \\\n",
      "0            Aachen             1    Valid    L5      21.0      Fell   \n",
      "1            Aarhus             2    Valid    H6     720.0      Fell   \n",
      "1111          Abajo             4    Valid    H5     331.0     Found   \n",
      "1113         Abbott             5    Valid  H3-6   21100.0     Found   \n",
      "2              Abee             6    Valid   EH4  107000.0      Fell   \n",
      "\n",
      "      fall_year  latitude  longitude           GeoLocation  class_id  \\\n",
      "0          1880  50.77500    6.08333     (50.775, 6.08333)         1   \n",
      "1          1951  56.18333   10.23333  (56.18333, 10.23333)         2   \n",
      "1111       1982  26.80000 -105.41667    (26.8, -105.41667)         4   \n",
      "1113       1951  36.30000 -104.28333    (36.3, -104.28333)         5   \n",
      "2          1952  54.21667 -113.00000    (54.21667, -113.0)         6   \n",
      "\n",
      "      location_id  \n",
      "0               1  \n",
      "1               2  \n",
      "1111            4  \n",
      "1113            5  \n",
      "2               6  \n"
     ]
    }
   ],
   "source": [
    "df[\"class_id\"] = df[\"meteorite_id\"]\n",
    "df[\"location_id\"] = df[\"meteorite_id\"]\n",
    "\n",
    "# Verify the new columns\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "e61645a7-0cae-42e3-ab40-ae793c54e0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 38114 entries, 0 to 30784\n",
      "Data columns (total 12 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   meteorite_name  38114 non-null  object \n",
      " 1   meteorite_id    38114 non-null  int64  \n",
      " 2   nametype        38114 non-null  object \n",
      " 3   class           38114 non-null  object \n",
      " 4   mass            38114 non-null  float64\n",
      " 5   fall_type       38114 non-null  object \n",
      " 6   fall_year       38114 non-null  int64  \n",
      " 7   latitude        38114 non-null  float64\n",
      " 8   longitude       38114 non-null  float64\n",
      " 9   GeoLocation     38114 non-null  object \n",
      " 10  class_id        38114 non-null  int64  \n",
      " 11  location_id     38114 non-null  int64  \n",
      "dtypes: float64(3), int64(4), object(5)\n",
      "memory usage: 3.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "4144f3a2-7ce5-4bfe-b2cd-d5ad24dcdb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Locations DataFrame shape: (45716, 5)\n",
      "   location_id  latitude  longitude         region  country\n",
      "0            1  50.77500    6.08333         Europe  Germany\n",
      "1            2  56.18333   10.23333         Europe  Germany\n",
      "2            6  54.21667 -113.00000          Other  Unknown\n",
      "3           10  16.88333  -99.90000          Other  Unknown\n",
      "4          370 -33.16667  -64.95000          Other  Unknown\n",
      "5          379  32.10000   71.80000           Asia    China\n",
      "6          390  44.83333   95.16667           Asia    China\n",
      "7          392  44.21667    0.61667         Europe  Germany\n",
      "8          398 -31.60000  -65.23333  South America   Brazil\n",
      "9          417 -30.86667  -64.55000  South America   Brazil\n",
      "Locations table created successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Read only the id + raw lat/lon from the original CSV\n",
    "raw_loc = pd.read_csv(\n",
    "    \"Meteorite_Landings.csv\",\n",
    "    usecols=[\"id\", \"reclat\", \"reclong\"]\n",
    ").rename(columns={\n",
    "    \"id\": \"meteorite_id\",\n",
    "    \"reclat\": \"latitude\",\n",
    "    \"reclong\": \"longitude\"\n",
    "})\n",
    "\n",
    "# 2. Your region/country helper\n",
    "def approximate_region_country(lat, lon):\n",
    "    if 24.5 <= lat <= 49.0 and -125.0 <= lon <= -66.9:\n",
    "        return \"North America\", \"USA\"\n",
    "    if -33.0 <= lat <= 5.0 and -74.0 <= lon <= -34.0:\n",
    "        return \"South America\", \"Brazil\"\n",
    "    if 35.0 <= lat <= 71.0 and -10.0 <= lon <= 40.0:\n",
    "        return \"Europe\", \"Germany\"\n",
    "    if -44.0 <= lat <= -10.0 and 112.0 <= lon <= 154.0:\n",
    "        return \"Oceania\", \"Australia\"\n",
    "    if 20.0 <= lat <= 55.0 and 60.0 <= lon <= 100.0:\n",
    "        return \"Asia\", \"China\"\n",
    "    if -35.0 <= lat <= 37.0 and -17.0 <= lon <= 51.0:\n",
    "        return \"Africa\", \"Egypt\"\n",
    "    if lat > 70:\n",
    "        return \"Arctic\", \"Arctic Region\"\n",
    "    if lat < -60:\n",
    "        return \"Antarctica\", \"Antarctica\"\n",
    "    if 10.0 <= lat <= 20.0 and -85.0 <= lon <= -60.0:\n",
    "        return \"Caribbean\", \"Caribbean Sea\"\n",
    "    if -10.0 <= lat <= 10.0 and 95.0 <= lon <= 141.0:\n",
    "        return \"Southeast Asia\", \"Indonesia\"\n",
    "    return \"Other\", \"Unknown\"\n",
    "\n",
    "# 3. Build locations_data by iterating over raw_loc\n",
    "locations_data = []\n",
    "for row in raw_loc.itertuples(index=False):\n",
    "    lat, lon = row.latitude, row.longitude\n",
    "    if pd.isnull(lat) or pd.isnull(lon):\n",
    "        region, country = \"Other\", \"Unknown\"\n",
    "    else:\n",
    "        region, country = approximate_region_country(lat, lon)\n",
    "    locations_data.append({\n",
    "        \"location_id\": row.meteorite_id,\n",
    "        \"latitude\":    lat,\n",
    "        \"longitude\":   lon,\n",
    "        \"region\":      region,\n",
    "        \"country\":     country\n",
    "    })\n",
    "\n",
    "# 4. Convert and write\n",
    "locations_df = pd.DataFrame(locations_data)\n",
    "print(\"Locations DataFrame shape:\", locations_df.shape)\n",
    "print(locations_df.head(10))\n",
    "locations_df.to_csv(\"locations.csv\", index=False)\n",
    "print(\"Locations table created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "2d994b9f-e4ca-495b-96e9-9ecd12b163e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save the cleaned DataFrame to a CSV file\n",
    "\n",
    "\n",
    "# ─── 1) BUILD CLASSIFICATION BEFORE dropping `class` ───\n",
    "classification_df = (\n",
    "    df[[\"class_id\",\"class\"]]\n",
    "      .drop_duplicates()\n",
    "      .rename(columns={\"class\":\"class_name\"}))\n",
    "classification_df.to_csv(\"meteorite_classification.csv\", index=False)\n",
    "\n",
    "df.drop(columns=[\"class\",\"latitude\",\"longitude\"], inplace=True)\n",
    "\n",
    "df.to_csv(\"cleaned_meteorite_landings.csv\", index=False)\n",
    "\n",
    "print(\"Cleaned dataset saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "45eab5b5-0391-4df2-91e1-5c5e69271b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "9d6c7e7e-0076-49a8-9b62-9fde305a84a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Institutions DataFrame shape: (38114, 3)\n",
      "   institution_id                             institution_name  \\\n",
      "0               1              National Institute of Astronomy   \n",
      "1               2                 College of Planetary Science   \n",
      "2               3           National Institute of Geochemistry   \n",
      "3               4              International Lab for Astronomy   \n",
      "4               5      National Institute of Planetary Science   \n",
      "5               6                   University of Geochemistry   \n",
      "6               7      International Lab for Planetary Science   \n",
      "7               8                         Academy of Astronomy   \n",
      "8               9      International Lab for Space Exploration   \n",
      "9              10  Global Research Center of Planetary Science   \n",
      "\n",
      "  institution_country  \n",
      "0               India  \n",
      "1              Canada  \n",
      "2             Germany  \n",
      "3                 USA  \n",
      "4               China  \n",
      "5               Japan  \n",
      "6             Denmark  \n",
      "7           Australia  \n",
      "8               India  \n",
      "9               Spain  \n"
     ]
    }
   ],
   "source": [
    "# Expanded list of countries\n",
    "countries = [\n",
    "    \"USA\", \"UK\", \"Germany\", \"France\", \"Canada\", \"Australia\", \"Japan\", \"China\", \"India\", \"Brazil\",\n",
    "    \"Spain\", \"Italy\", \"Sweden\", \"Norway\", \"Denmark\", \"Finland\", \"Netherlands\", \"Belgium\", \"Switzerland\", \"Austria\"\n",
    "]\n",
    "\n",
    "# Possible prefixes for institution names\n",
    "institution_prefixes = [\n",
    "    \"University of\", \"National Institute of\", \"Global Research Center of\",\n",
    "    \"College of\", \"Academy of\", \"Royal Institute of\", \"International Lab for\",\n",
    "    \"Center for Advanced\"\n",
    "]\n",
    "\n",
    "# Possible fields for institution names\n",
    "institution_fields = [\n",
    "    \"Astronomy\", \"Geoscience\", \"Meteorite Studies\", \"Planetary Science\",\n",
    "    \"Cosmic Research\", \"Space Exploration\", \"Astrophysics\", \"Geology\", \"Geochemistry\"\n",
    "]\n",
    "\n",
    "# Number of institutions you want\n",
    "num_institutions = 38114\n",
    "\n",
    "institutions_data = []\n",
    "for i in range(1, num_institutions + 1):\n",
    "    # Randomly build an institution name by combining prefix + field\n",
    "    prefix = random.choice(institution_prefixes)\n",
    "    field = random.choice(institution_fields)\n",
    "    institution_name = f\"{prefix} {field}\"\n",
    "    \n",
    "    # Randomly pick a country\n",
    "    institution_country = random.choice(countries)\n",
    "    \n",
    "    institutions_data.append({\n",
    "        \"institution_id\": i,\n",
    "        \"institution_name\": institution_name,\n",
    "        \"institution_country\": institution_country\n",
    "    })\n",
    "\n",
    "institutions_df = pd.DataFrame(institutions_data)\n",
    "\n",
    "print(\"Institutions DataFrame shape:\", institutions_df.shape)\n",
    "print(institutions_df.head(10))  # Show the first 10 rows for variety\n",
    "\n",
    "# Optionally, save to CSV\n",
    "institutions_df.to_csv(\"institutions.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "0e7147bf-79a7-4491-af81-c2aebdb1cef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scientists DataFrame shape: (38114, 4)\n",
      "   scientist_id     scientist_name  institution_id  \\\n",
      "0             1        Naomi Gomez           14706   \n",
      "1             2        Eva Morales            5977   \n",
      "2             3      Zack Thompson            5123   \n",
      "3             4  Isabella Thompson           12391   \n",
      "4             5        Ethan Green           22064   \n",
      "5             6        Javier King           31748   \n",
      "6             7        Uma Johnson            9801   \n",
      "7             8    Priyanka Taylor              54   \n",
      "8             9     Ethan Thompson           17380   \n",
      "9            10      George Taylor           15543   \n",
      "\n",
      "                          email  \n",
      "0        naomi.gomez1@gmail.com  \n",
      "1        eva.morales2@gmail.com  \n",
      "2      zack.thompson3@gmail.com  \n",
      "3  isabella.thompson4@gmail.com  \n",
      "4        ethan.green5@gmail.com  \n",
      "5        javier.king6@gmail.com  \n",
      "6        uma.johnson7@gmail.com  \n",
      "7    priyanka.taylor8@gmail.com  \n",
      "8     ethan.thompson9@gmail.com  \n",
      "9     george.taylor10@gmail.com  \n",
      "\n",
      "Scientists table created successfully!\n"
     ]
    }
   ],
   "source": [
    "# We'll assume you already have institutions_df with:\n",
    "# columns = [\"institution_id\", \"institution_name\", \"institution_country\"]\n",
    "# and institution_id goes from 1 to 3000.\n",
    "\n",
    "num_scientists = 38114\n",
    "\n",
    "# Example lists of first and last names\n",
    "# A list of 100 diverse first names\n",
    "first_names = [\n",
    "    \"Alice\", \"Bob\", \"Carol\", \"David\", \"Eva\", \"Frank\", \"Grace\", \"Henry\", \"Irene\", \"Jack\",\n",
    "    \"Kathy\", \"Larry\", \"Megan\", \"Nathan\", \"Olivia\", \"Paul\", \"Quincy\", \"Rachel\", \"Sam\", \"Tina\",\n",
    "    \"Ursula\", \"Victor\", \"Wendy\", \"Xander\", \"Yvonne\", \"Zack\", \"Amira\", \"Bao\", \"Carmen\", \"Darius\",\n",
    "    \"Ethan\", \"Fatima\", \"George\", \"Hannah\", \"Isabella\", \"Jamal\", \"Kofi\", \"Laila\", \"Mohammed\", \"Naomi\",\n",
    "    \"Omar\", \"Priya\", \"Quinn\", \"Ravi\", \"Sofia\", \"Tariq\", \"Uma\", \"Viktor\", \"Xia\", \"Yara\",\n",
    "    \"Zainab\", \"Abdul\", \"Bella\", \"Chloe\", \"Diego\", \"Elena\", \"Felix\", \"Gabriel\", \"Harper\", \"Ibrahim\",\n",
    "    \"Jasmine\", \"Keiko\", \"Luis\", \"Mariana\", \"Nina\", \"Oscar\", \"Penelope\", \"Rashid\", \"Sara\", \"Tomas\",\n",
    "    \"Umar\", \"Valentina\", \"Willow\", \"Ximena\", \"Yusuf\", \"Zara\", \"Aisha\", \"Brandon\", \"Ciara\", \"Dmitri\",\n",
    "    \"Elijah\", \"Fiona\", \"Giovanni\", \"Helena\", \"Ivy\", \"Javier\", \"Kaori\", \"Leonardo\", \"Maya\", \"Nadia\",\n",
    "    \"Orlando\", \"Priyanka\", \"Riley\", \"Santiago\", \"Tatiana\", \"Uriel\", \"Veronica\", \"Xavier\", \"Yasmin\", \"Zoe\"\n",
    "]\n",
    "\n",
    "# A list of 100 diverse last names\n",
    "last_names = [\n",
    "    \"Smith\", \"Johnson\", \"Williams\", \"Brown\", \"Jones\", \"Miller\", \"Davis\", \"Garcia\", \"Rodriguez\", \"Wilson\",\n",
    "    \"Martinez\", \"Anderson\", \"Taylor\", \"Thomas\", \"Hernandez\", \"Moore\", \"Martin\", \"Jackson\", \"Thompson\", \"White\",\n",
    "    \"Lopez\", \"Lee\", \"Gonzalez\", \"Harris\", \"Clark\", \"Lewis\", \"Robinson\", \"Walker\", \"Perez\", \"Hall\",\n",
    "    \"Wright\", \"King\", \"Scott\", \"Green\", \"Baker\", \"Adams\", \"Nelson\", \"Hill\", \"Ramirez\", \"Campbell\",\n",
    "    \"Mitchell\", \"Roberts\", \"Carter\", \"Phillips\", \"Evans\", \"Turner\", \"Torres\", \"Parker\", \"Collins\", \"Edwards\",\n",
    "    \"Stewart\", \"Flores\", \"Morris\", \"Nguyen\", \"Murphy\", \"Rivera\", \"Cook\", \"Rogers\", \"Morgan\", \"Peterson\",\n",
    "    \"Cooper\", \"Reed\", \"Bailey\", \"Bell\", \"Gomez\", \"Kelly\", \"Howard\", \"Ward\", \"Cox\", \"Diaz\",\n",
    "    \"Richardson\", \"Wood\", \"Watson\", \"Brooks\", \"Bennett\", \"Gray\", \"James\", \"Reyes\", \"Cruz\", \"Hughes\",\n",
    "    \"Price\", \"Myers\", \"Long\", \"Foster\", \"Sanders\", \"Ross\", \"Morales\", \"Powell\", \"Sullivan\", \"Russell\",\n",
    "    \"Ortiz\", \"Jenkins\", \"Gutierrez\", \"Perry\", \"Butler\", \"Barnes\", \"Fisher\", \"Henderson\", \"Coleman\", \"Simmons\"\n",
    "]\n",
    "\n",
    "scientists_data = []\n",
    "\n",
    "# If you want to ensure you reference the actual IDs in institutions_df:\n",
    "institution_ids = institutions_df[\"institution_id\"].tolist()\n",
    "\n",
    "for i in range(1, num_scientists + 1):\n",
    "    first = random.choice(first_names)\n",
    "    last = random.choice(last_names)\n",
    "    name = f\"{first} {last}\"\n",
    "    \n",
    "    # Randomly pick an institution_id from the Institutions table\n",
    "    inst_id = random.choice(institution_ids)\n",
    "    \n",
    "    # Generate a synthetic email\n",
    "    email = f\"{first.lower()}.{last.lower()}{i}@gmail.com\"\n",
    "    \n",
    "    scientists_data.append({\n",
    "        \"scientist_id\": i,\n",
    "        \"scientist_name\": name,\n",
    "        \"institution_id\": inst_id,\n",
    "        \"email\": email\n",
    "    })\n",
    "\n",
    "scientists_df = pd.DataFrame(scientists_data)\n",
    "\n",
    "print(\"Scientists DataFrame shape:\", scientists_df.shape)\n",
    "print(scientists_df.head(10))\n",
    "\n",
    "# Optional: save to CSV\n",
    "scientists_df.to_csv(\"scientists.csv\", index=False)\n",
    "print(\"\\nScientists table created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "f5270545-00f9-44e2-be47-b10440e9c267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Findings DataFrame shape: (38114, 4)\n",
      "   finding_id  meteorite_id                            findings  \\\n",
      "0           1             1                       Silicate rich   \n",
      "1           2             2              Low volatile compounds   \n",
      "2           3             4           Trace rare earth elements   \n",
      "3           4             5  Presence of water-bearing minerals   \n",
      "4           5             6       Iron-nickel alloy composition   \n",
      "5           6             7               High chromium content   \n",
      "6           7             8            Unique mineral structure   \n",
      "7           8             9       Shock-metamorphosed structure   \n",
      "8           9            10       Iron-nickel alloy composition   \n",
      "9          10            11                Porous texture noted   \n",
      "\n",
      "   discovery_year  \n",
      "0            1880  \n",
      "1            1951  \n",
      "2            1982  \n",
      "3            1951  \n",
      "4            1952  \n",
      "5            1941  \n",
      "6            1840  \n",
      "7            1997  \n",
      "8            1976  \n",
      "9            1989  \n",
      "\n",
      "Findings table created successfully with no NaN values!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# 1. Build a quick lookup: meteorite_id -> fall_year from df\n",
    "year_lookup = df.set_index(\"meteorite_id\")[\"fall_year\"].to_dict()\n",
    "\n",
    "# 2. Set the number of finding records equal to the number of rows in df\n",
    "num_findings = len(df)  # ensures one finding per meteorite row\n",
    "\n",
    "# 3. Possible \"findings\" text\n",
    "findings_options = [\n",
    "    \"Nickel traces found\",\n",
    "    \"Unusual chemical makeup\",\n",
    "    \"High metal content\",\n",
    "    \"Low density\",\n",
    "    \"Carbonaceous composition\",\n",
    "    \"Silicate rich\",\n",
    "    \"High iron content\",\n",
    "    \"Magnetic properties observed\",\n",
    "    \"Rare isotopes detected\",\n",
    "    \"Significant sulfur levels\",\n",
    "    \"Anomalous oxygen isotope ratio\",\n",
    "    \"Presence of organic compounds\",\n",
    "    \"High chromium content\",\n",
    "    \"Elevated manganese levels\",\n",
    "    \"Trace platinum group metals\",\n",
    "    \"Low volatile compounds\",\n",
    "    \"Unique mineral structure\",\n",
    "    \"Crystalline texture observed\",\n",
    "    \"Shock-metamorphosed structure\",\n",
    "    \"Presence of water-bearing minerals\",\n",
    "    \"Exotic mineral inclusions\",\n",
    "    \"Abundant olivine crystals\",\n",
    "    \"Anomalous silicon content\",\n",
    "    \"High magnesium content\",\n",
    "    \"Significant phosphorous levels\",\n",
    "    \"Evidence of partial melting\",\n",
    "    \"Porous texture noted\",\n",
    "    \"High titanium concentration\",\n",
    "    \"Iron-nickel alloy composition\",\n",
    "    \"Low carbon content\",\n",
    "    \"Elevated cobalt levels\",\n",
    "    \"Trace rare earth elements\"\n",
    "]\n",
    "\n",
    "\n",
    "# 4. Collect the list of valid meteorite IDs from df\n",
    "meteorite_ids = df[\"meteorite_id\"].tolist()\n",
    "\n",
    "# 5. Generate the Findings data so that every meteorite has one record\n",
    "findings_data = []\n",
    "for i, m_id in enumerate(meteorite_ids, start=1):\n",
    "    # Use the corresponding fall_year for this meteorite_id\n",
    "    discovery_year = year_lookup[m_id]\n",
    "    \n",
    "    findings_data.append({\n",
    "        \"finding_id\": i,\n",
    "        \"meteorite_id\": m_id,\n",
    "        \"findings\": random.choice(findings_options),\n",
    "        \"discovery_year\": discovery_year\n",
    "    })\n",
    "\n",
    "# 6. Create the Findings DataFrame\n",
    "findings_df = pd.DataFrame(findings_data)\n",
    "\n",
    "# 7. Inspect the Findings DataFrame\n",
    "print(\"Findings DataFrame shape:\", findings_df.shape)\n",
    "print(findings_df.head(10))\n",
    "print(\"\\nFindings table created successfully with no NaN values!\")\n",
    "findings_df.to_csv(\"findings.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "739964d4-ac38-411e-bafc-e51ab52d92e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funding DataFrame shape: (38114, 5)\n",
      "   funding_id  scientist_id  amount         sponsor  year\n",
      "0           1          3531  175476            CNSA  1960\n",
      "1           2         37053  305660        Raytheon  1967\n",
      "2           3          8877  377434        Momentus  2024\n",
      "3           4            65  153250     Axiom Space  1996\n",
      "4           5         33223  107847        Eutelsat  1930\n",
      "5           6         23511  310861             MDA  1990\n",
      "6           7         19703  284762    Interstellar  1997\n",
      "7           8          2684  247397  Exos Aerospace  1941\n",
      "8           9         36064  148904        L3Harris  1936\n",
      "9          10         38114  430733         Orbcomm  1903\n",
      "\n",
      "Funding table created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Number of funding records you want\n",
    "num_funding = 38114\n",
    "\n",
    "# Possible sponsors\n",
    "sponsors = [\n",
    "    \"NASA\", \"NSF\", \"ESA\", \"Private\", \"University\",\n",
    "    \"SpaceX\", \"Blue Origin\", \"Roscosmos\", \"CNSA\", \"ISRO\",\n",
    "    \"JAXA\", \"CNES\", \"ASI\", \"KARI\", \"Sierra Nevada Corporation\",\n",
    "    \"Lockheed Martin\", \"Boeing\", \"Airbus\", \"Northrop Grumman\", \"Raytheon\",\n",
    "    \"General Dynamics\", \"Virgin Galactic\", \"SpaceIL\", \"Orbital ATK\", \"Thales Alenia Space\",\n",
    "    \"Mitsubishi Heavy Industries\", \"Arianespace\", \"Dynetics\", \"Relativity Space\", \"Rocket Lab\",\n",
    "    \"Firefly Aerospace\", \"PLD Space\", \"ExPace\", \"Effective Innovations\", \"Xplore\",\n",
    "    \"Axiom Space\", \"MDA\", \"Maxar Technologies\", \"SES\", \"Intelsat\",\n",
    "    \"Inmarsat\", \"Eutelsat\", \"Telesat\", \"OneWeb\", \"Hughes\",\n",
    "    \"Globalstar\", \"Iridium\", \"Viasat\", \"Planet Labs\", \"Spire Global\",\n",
    "    \"Astrocast\", \"Fleet Space Technologies\", \"GomSpace\", \"Kepler Communications\", \"Swarm Technologies\",\n",
    "    \"Vector Launch\", \"Orbit Fab\", \"Made In Space\", \"Momentus\", \"SpacePharma\",\n",
    "    \"Zero 2 Infinity\", \"Blue Canyon Technologies\", \"Tyvak Nano-Satellite Systems\", \"Aerospace Corporation\", \"Boeing Defense\",\n",
    "    \"Lockheed Martin Space\", \"Northrop Grumman Innovation Systems\", \"Space Systems Loral\", \"SSL\", \"Paragon Space Development Corporation\",\n",
    "    \"Bigelow Aerospace\", \"Aerojet Rocketdyne\", \"AeroVironment\", \"Ball Aerospace\", \"Cobham\",\n",
    "    \"Cubic Corporation\", \"Diehl Aerospace\", \"Harris Corporation\", \"ITAR Solutions\", \"L3Harris\",\n",
    "    \"OHB System\", \"Orbcomm\", \"Planetary Resources\", \"Rocket Crafters\", \"Space Adventures\",\n",
    "    \"Space Angels\", \"Space Foundation\", \"Space Generation Advisory Council\", \"Society of Exploration Geophysicists\", \"The Planetary Society\",\n",
    "    \"United Launch Alliance\", \"Vulcan Inc.\", \"Zero Gravity Solutions\", \"Exos Aerospace\", \"Astrobotic\",\n",
    "    \"Firefly Rocket Technologies\", \"Relativity\", \"Interstellar\", \"NewSpace Capital\", \"Quantum Space\"\n",
    "]\n",
    "\n",
    "\n",
    "# Collect a list of valid scientist IDs from scientists_df\n",
    "scientist_ids = scientists_df[\"scientist_id\"].tolist()\n",
    "\n",
    "funding_data = []\n",
    "for i in range(1, num_funding + 1):\n",
    "    funding_data.append({\n",
    "        \"funding_id\": i,\n",
    "        \"scientist_id\": random.choice(scientist_ids),\n",
    "        \"amount\": random.randint(100000, 500000),\n",
    "        \"sponsor\": random.choice(sponsors),\n",
    "        \"year\": random.randint(1900, 2025)\n",
    "    })\n",
    "\n",
    "funding_df = pd.DataFrame(funding_data)\n",
    "\n",
    "print(\"Funding DataFrame shape:\", funding_df.shape)\n",
    "print(funding_df.head(10))\n",
    "\n",
    "# Optionally, save to CSV\n",
    "funding_df.to_csv(\"funding.csv\", index=False)\n",
    "print(\"\\nFunding table created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "ecc01150-8b26-49a6-b914-b6ef449e913b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Studies DataFrame shape: (38114, 4)\n",
      "   study_id  meteorite_id  scientist_id  study_date\n",
      "0         1         14290         15519  2025-10-08\n",
      "1         2         53592         33660  2019-06-14\n",
      "2         3         24241         34891  2012-08-25\n",
      "3         4          3580         36047  2018-02-07\n",
      "4         5         20265          4053  2005-10-10\n",
      "5         6          3597         30076  2012-09-28\n",
      "6         7         16857          6119  2011-04-05\n",
      "7         8          8042         27624  2015-08-28\n",
      "8         9         46695          8126  2016-05-15\n",
      "9        10         18189          9384  2008-02-15\n",
      "\n",
      "Studies table created successfully!\n"
     ]
    }
   ],
   "source": [
    "# We assume you have these DataFrames already in memory:\n",
    "# findings_df (columns: finding_id, meteorite_id, findings, discovery_year)\n",
    "# scientists_df (columns: scientist_id, scientist_name, institution_id, email)\n",
    "\n",
    "# 1. Convert findings_df to a list of dictionaries for easy random sampling\n",
    "findings_rows = findings_df.to_dict(\"records\")\n",
    "\n",
    "# 2. Collect all scientist IDs for random assignment\n",
    "scientist_ids = scientists_df[\"scientist_id\"].tolist()\n",
    "\n",
    "# 3. Decide how many \"study\" records you want\n",
    "num_studies = 38114\n",
    "\n",
    "# 4. Date range for random study_date\n",
    "start_date = datetime(2000, 1, 1)\n",
    "end_date = datetime(2025, 12, 31)\n",
    "\n",
    "def random_date(start, end):\n",
    "    \"\"\"Generate a random date between 'start' and 'end'.\"\"\"\n",
    "    delta = end - start\n",
    "    random_days = random.randrange(delta.days)\n",
    "    return start + timedelta(days=random_days)\n",
    "\n",
    "studies_data = []\n",
    "\n",
    "for i in range(1, num_studies + 1):\n",
    "    # Pick a random row from the existing Findings table\n",
    "    finding_row = random.choice(findings_rows)\n",
    "    # This ensures the study references the same meteorite and findings text\n",
    "\n",
    "    # Pick a random scientist\n",
    "    s_id = random.choice(scientist_ids)\n",
    "\n",
    "    # Generate a random date\n",
    "    study_dt = random_date(start_date, end_date).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # Build the Studies record\n",
    "    studies_data.append({\n",
    "        \"study_id\": i,\n",
    "        \"meteorite_id\": finding_row[\"meteorite_id\"],  # same meteorite as the chosen finding\n",
    "        \"scientist_id\": s_id,\n",
    "        \"study_date\": study_dt,\n",
    "        \"findings\": finding_row[\"findings\"]  # reuse the same text from Findings\n",
    "    })\n",
    "\n",
    "# 5. Convert to DataFrame\n",
    "studies_df = pd.DataFrame(studies_data)\n",
    "studies_df.drop(columns=[\"findings\"], inplace=True)\n",
    "# 6. Inspect\n",
    "print(\"Studies DataFrame shape:\", studies_df.shape)\n",
    "print(studies_df.head(10))\n",
    "print(\"\\nStudies table created successfully!\")\n",
    "\n",
    "# 7. (Optional) save to CSV\n",
    "studies_df.to_csv(\"studies.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "df1164a9-cd84-4adb-a9f5-4db532ca6848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Research Papers DataFrame shape: (38114, 5)\n",
      "   paper_id                               title publication_date  \\\n",
      "0         1  New Insights into Meteorite #27953       2025-12-19   \n",
      "1         2  New Insights into Meteorite #44901       2022-11-30   \n",
      "2         3  New Insights into Meteorite #49437       2012-04-12   \n",
      "3         4  New Insights into Meteorite #41695       2018-08-18   \n",
      "4         5  New Insights into Meteorite #38554       2020-09-26   \n",
      "5         6  New Insights into Meteorite #31227       2023-08-26   \n",
      "6         7  New Insights into Meteorite #13783       2008-03-08   \n",
      "7         8  New Insights into Meteorite #28153       2025-05-13   \n",
      "8         9  New Insights into Meteorite #23026       2023-11-15   \n",
      "9        10  New Insights into Meteorite #34743       2008-09-27   \n",
      "\n",
      "                                journal_name  scientist_id  \n",
      "0                   Astroinformatics Journal           899  \n",
      "1                 Astronomy Research Journal         18876  \n",
      "2                      Intergalactic Studies         23572  \n",
      "3                Cosmology and Space Science         26667  \n",
      "4                Journal of Modern Astronomy         30643  \n",
      "5  Journal of Space Exploration and Research         37518  \n",
      "6                 Space, Stars, and Galaxies         33175  \n",
      "7         Journal of Extragalactic Astronomy         36026  \n",
      "8                     Interplanetary Science         17032  \n",
      "9                Planetary and Space Science         26434  \n"
     ]
    }
   ],
   "source": [
    "# We'll assume you already have a DataFrame called scientists_df \n",
    "# with at least one column \"scientist_id\".\n",
    "\n",
    "# Number of research papers you want\n",
    "num_papers = 38114\n",
    "\n",
    "# Example list of journal names\n",
    "journals = [\n",
    "    \"Astrophysical Journal\",\n",
    "    \"Nature\",\n",
    "    \"Science\",\n",
    "    \"Journal of Meteorites\",\n",
    "    \"Geochimica et Cosmochimica Acta\",\n",
    "    \"International Journal of Astronomy\",\n",
    "    \"Space Science Reviews\",\n",
    "    \"Journal of Planetary Science\",\n",
    "    \"Astrophysics and Space Science\",\n",
    "    \"Cosmic Research Journal\",\n",
    "    \"Journal of Astrophysics\",\n",
    "    \"Advances in Astronomy\",\n",
    "    \"Journal of Space Exploration\",\n",
    "    \"Journal of Cosmology\",\n",
    "    \"Cosmochemistry and Planetary Materials\",\n",
    "    \"Planetary and Space Science\",\n",
    "    \"Astrochemistry Journal\",\n",
    "    \"Space and Time Journal\",\n",
    "    \"Journal of Exoplanet Studies\",\n",
    "    \"Journal of Cosmic Evolution\",\n",
    "    \"International Journal of Astrophysics\",\n",
    "    \"Astrobiology Reviews\",\n",
    "    \"Journal of Celestial Mechanics\",\n",
    "    \"Space Exploration Today\",\n",
    "    \"Advances in Space Research\",\n",
    "    \"Journal of Cosmic Rays\",\n",
    "    \"Galactic Science Journal\",\n",
    "    \"Interstellar Research Letters\",\n",
    "    \"Journal of Planetary Research\",\n",
    "    \"Solar Physics Journal\",\n",
    "    \"Journal of Astronomical Instrumentation\",\n",
    "    \"Stellar Astronomy Journal\",\n",
    "    \"Journal of Cosmological Studies\",\n",
    "    \"Journal of Space and Astronomy\",\n",
    "    \"Interplanetary Science\",\n",
    "    \"Astronomy and Astrophysics\",\n",
    "    \"Space Weather and Climate\",\n",
    "    \"Journal of Solar and Stellar Studies\",\n",
    "    \"Journal of Astrophysical Techniques\",\n",
    "    \"Journal of Astronomical Observations\",\n",
    "    \"Cosmic Horizons Journal\",\n",
    "    \"Journal of Extragalactic Astronomy\",\n",
    "    \"Deep Space Journal\",\n",
    "    \"Journal of Astrostatistics\",\n",
    "    \"Space Science and Technology\",\n",
    "    \"Astrophysical Research Letters\",\n",
    "    \"Journal of Astronomical Sciences\",\n",
    "    \"Planetary Science and Exploration\",\n",
    "    \"Journal of Celestial Observations\",\n",
    "    \"Intergalactic Studies\",\n",
    "    \"Journal of Modern Astronomy\",\n",
    "    \"Cosmology Today\",\n",
    "    \"Advances in Planetary Science\",\n",
    "    \"Astrogeology Journal\",\n",
    "    \"Journal of Space and Time\",\n",
    "    \"Journal of Cosmic Chemistry\",\n",
    "    \"Exoplanet Research Journal\",\n",
    "    \"Journal of Stellar Evolution\",\n",
    "    \"Space Exploration and Technology\",\n",
    "    \"Cosmic Horizons Review\",\n",
    "    \"Journal of Observational Astronomy\",\n",
    "    \"Astrophysical Journal Letters\",\n",
    "    \"Journal of Interstellar Studies\",\n",
    "    \"Galactic Astronomy\",\n",
    "    \"Journal of Celestial Research\",\n",
    "    \"Astroinformatics Journal\",\n",
    "    \"Journal of Space Engineering\",\n",
    "    \"Astrophysics Reports\",\n",
    "    \"Journal of Solar Research\",\n",
    "    \"Journal of Cosmic Phenomena\",\n",
    "    \"International Journal of Space Science\",\n",
    "    \"Journal of Astronomical Research\",\n",
    "    \"Advances in Cosmic Studies\",\n",
    "    \"Journal of Planetary Atmospheres\",\n",
    "    \"Space Science and Exploration\",\n",
    "    \"Journal of Deep Space Research\",\n",
    "    \"Cosmology and Gravitation\",\n",
    "    \"Journal of Stellar Dynamics\",\n",
    "    \"Journal of Planetary Systems\",\n",
    "    \"Astrophysical Methods\",\n",
    "    \"Journal of Interplanetary Research\",\n",
    "    \"Journal of Space Physics\",\n",
    "    \"Cosmic Evolution Review\",\n",
    "    \"Journal of Extragalactic Research\",\n",
    "    \"Interstellar Astronomy\",\n",
    "    \"Journal of Cosmic Origins\",\n",
    "    \"Space and Universe Journal\",\n",
    "    \"Advances in Interstellar Studies\",\n",
    "    \"Journal of Space Exploration and Research\",\n",
    "    \"Astronomy Research Journal\",\n",
    "    \"Journal of Cosmic Energy\",\n",
    "    \"Journal of Planetary Climates\",\n",
    "    \"Space Science Insights\",\n",
    "    \"Journal of Astrophysical Phenomena\",\n",
    "    \"Cosmic Exploration Journal\",\n",
    "    \"Journal of Celestial Phenomena\",\n",
    "    \"Journal of Universe Studies\",\n",
    "    \"Space, Stars, and Galaxies\",\n",
    "    \"Journal of Cosmic Investigations\",\n",
    "    \"Astronomical Research and Reviews\",\n",
    "    \"International Journal of Space Exploration\",\n",
    "    \"Journal of Advanced Astronomy\",\n",
    "    \"Space and Beyond Journal\",\n",
    "    \"Journal of Galactic Research\",\n",
    "    \"Cosmology and Space Science\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# Collect scientist IDs from your existing scientists_df\n",
    "scientist_ids = scientists_df[\"scientist_id\"].tolist()\n",
    "\n",
    "# Helper function to generate a random date between two boundaries\n",
    "def random_date(start, end):\n",
    "    delta = end - start\n",
    "    random_days = random.randrange(delta.days)\n",
    "    return start + timedelta(days=random_days)\n",
    "\n",
    "start_date = datetime(2000, 1, 1)\n",
    "end_date = datetime(2025, 12, 31)\n",
    "\n",
    "papers_data = []\n",
    "for i in range(1, num_papers + 1):\n",
    "    # Generate a random publication date\n",
    "    pub_date = random_date(start_date, end_date).strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    # Optionally, create a random title referencing an integer or meteorite class\n",
    "    random_num = random.randint(1, 50000)\n",
    "    title = f\"New Insights into Meteorite #{random_num}\"\n",
    "    \n",
    "    # Pick a random scientist_id\n",
    "    s_id = random.choice(scientist_ids)\n",
    "    \n",
    "    # Build the paper record\n",
    "    papers_data.append({\n",
    "        \"paper_id\": i,\n",
    "        \"title\": title,\n",
    "        \"publication_date\": pub_date,\n",
    "        \"journal_name\": random.choice(journals),\n",
    "        \"scientist_id\": s_id\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "papers_df = pd.DataFrame(papers_data)\n",
    "\n",
    "# Inspect\n",
    "print(\"Research Papers DataFrame shape:\", papers_df.shape)\n",
    "print(papers_df.head(10))\n",
    "\n",
    "\n",
    "# (Optional) save to CSV\n",
    "papers_df.to_csv(\"research_papers.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "c0f2af84-074f-48a7-ac15-f2017f51ff41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 38114 entries, 0 to 30784\n",
      "Data columns (total 9 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   meteorite_name  38114 non-null  object \n",
      " 1   meteorite_id    38114 non-null  int64  \n",
      " 2   nametype        38114 non-null  object \n",
      " 3   mass            38114 non-null  float64\n",
      " 4   fall_type       38114 non-null  object \n",
      " 5   fall_year       38114 non-null  int64  \n",
      " 6   GeoLocation     38114 non-null  object \n",
      " 7   class_id        38114 non-null  int64  \n",
      " 8   location_id     38114 non-null  int64  \n",
      "dtypes: float64(1), int64(4), object(4)\n",
      "memory usage: 2.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "3580d74d-2d87-4606-908b-b59dc6c3d9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expeditions DataFrame shape: (38114, 5)\n",
      "   expedition_id  meteorite_id  scientist_id expedition_date  \\\n",
      "0              1         44719         23717      1959-03-21   \n",
      "1              2           799         16419      2024-11-05   \n",
      "2              3         20925          7113      1970-12-30   \n",
      "3              4          1607         13502      2018-01-13   \n",
      "4              5          7150         24220      1933-10-05   \n",
      "5              6         46788         25410      1947-08-16   \n",
      "6              7         11557         13217      1944-03-27   \n",
      "7              8         18832         31664      2017-04-10   \n",
      "8              9         21312         19291      1923-06-13   \n",
      "9             10          8850         33494      2009-04-08   \n",
      "\n",
      "                              notes  \n",
      "0        Field collection in forest  \n",
      "1             Deep cave exploration  \n",
      "2                Desert dune survey  \n",
      "3        Expedition in polar region  \n",
      "4            Underwater exploration  \n",
      "5             Deep cave exploration  \n",
      "6            Underwater exploration  \n",
      "7                  Urban expedition  \n",
      "8             Deep cave exploration  \n",
      "9  High-altitude balloon collection  \n",
      "\n",
      "Expeditions table created successfully with the same number of rows as the main DataFrame!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# We'll assume you already have:\n",
    "# - meteorites_df (or df) with a \"meteorite_id\" column\n",
    "# - scientists_df with a \"scientist_id\" column\n",
    "\n",
    "# 1. Set number of expedition records equal to the number of rows in your main DataFrame (df)\n",
    "num_expeditions = len(df)\n",
    "\n",
    "# 2. Get lists of valid meteorite and scientist IDs\n",
    "meteorite_ids = df[\"meteorite_id\"].tolist()\n",
    "scientist_ids = scientists_df[\"scientist_id\"].tolist()\n",
    "\n",
    "# 3. Helper function to generate random dates\n",
    "def random_date(start, end):\n",
    "    delta = end - start\n",
    "    random_days = random.randrange(delta.days)\n",
    "    return start + timedelta(days=random_days)\n",
    "\n",
    "start_date = datetime(1900, 1, 1)\n",
    "end_date = datetime(2025, 12, 31)\n",
    "\n",
    "# 4. Expanded list of possible expedition notes\n",
    "expedition_notes = [\n",
    "    \"Collected sample from desert\",\n",
    "    \"Expedition in polar region\",\n",
    "    \"Field collection in forest\",\n",
    "    \"Urban expedition\",\n",
    "    \"Underwater exploration\",\n",
    "    \"Mountain peak sampling\",\n",
    "    \"Volcanic crater sampling\",\n",
    "    \"Deep cave exploration\",\n",
    "    \"Glacial core drilling\",\n",
    "    \"Tropical rainforest expedition\",\n",
    "    \"Arid plateau mission\",\n",
    "    \"Archaeological site investigation\",\n",
    "    \"Remote island retrieval\",\n",
    "    \"Meteor crater excavation\",\n",
    "    \"High-altitude balloon collection\",\n",
    "    \"Ice shelf reconnaissance\",\n",
    "    \"Desert dune survey\"\n",
    "]\n",
    "\n",
    "# 5. Generate the Expeditions data\n",
    "expeditions_data = []\n",
    "for i in range(1, num_expeditions + 1):\n",
    "    m_id = random.choice(meteorite_ids)\n",
    "    s_id = random.choice(scientist_ids)\n",
    "    expedition_dt = random_date(start_date, end_date).strftime(\"%Y-%m-%d\")\n",
    "    note = random.choice(expedition_notes)\n",
    "    \n",
    "    expeditions_data.append({\n",
    "        \"expedition_id\": i,\n",
    "        \"meteorite_id\": m_id,\n",
    "        \"scientist_id\": s_id,\n",
    "        \"expedition_date\": expedition_dt,\n",
    "        \"notes\": note\n",
    "    })\n",
    "\n",
    "# 6. Create the Expeditions DataFrame\n",
    "expeditions_df = pd.DataFrame(expeditions_data)\n",
    "\n",
    "# 7. Inspect the result\n",
    "print(\"Expeditions DataFrame shape:\", expeditions_df.shape)\n",
    "print(expeditions_df.head(10))\n",
    "print(\"\\nExpeditions table created successfully with the same number of rows as the main DataFrame!\")\n",
    "\n",
    "# 8. (Optional) Save to CSV\n",
    "expeditions_df.to_csv(\"expeditions.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "e399c525-b3b1-42fc-afc4-f2e13d57ab11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information on metrodf:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 38114 entries, 0 to 30784\n",
      "Data columns (total 9 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   meteorite_name  38114 non-null  object \n",
      " 1   meteorite_id    38114 non-null  int64  \n",
      " 2   nametype        38114 non-null  object \n",
      " 3   mass            38114 non-null  float64\n",
      " 4   fall_type       38114 non-null  object \n",
      " 5   fall_year       38114 non-null  int64  \n",
      " 6   GeoLocation     38114 non-null  object \n",
      " 7   class_id        38114 non-null  int64  \n",
      " 8   location_id     38114 non-null  int64  \n",
      "dtypes: float64(1), int64(4), object(4)\n",
      "memory usage: 2.9+ MB\n"
     ]
    }
   ],
   "source": [
    "# Create a copy of the main DataFrame and name it metrodf\n",
    "metrodf = df.copy()\n",
    "\n",
    "# Display information about metrodf\n",
    "print(\"Information on metrodf:\")\n",
    "metrodf.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "d8837cf4-a06d-4fea-a5f0-eb543dca2250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information on metrodf:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 38114 entries, 0 to 30784\n",
      "Data columns (total 7 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   meteorite_id    38114 non-null  int64  \n",
      " 1   meteorite_name  38114 non-null  object \n",
      " 2   mass            38114 non-null  float64\n",
      " 3   fall_type       38114 non-null  object \n",
      " 4   fall_year       38114 non-null  int64  \n",
      " 5   class_id        38114 non-null  int64  \n",
      " 6   location_id     38114 non-null  int64  \n",
      "dtypes: float64(1), int64(4), object(2)\n",
      "memory usage: 2.3+ MB\n",
      "metrodf_cleaned.csv written successfully!\n"
     ]
    }
   ],
   "source": [
    "# Build metrodf by selecting only the fields you need\n",
    "metrodf = df[[\n",
    "    \"meteorite_id\",\n",
    "    \"meteorite_name\",\n",
    "    \"mass\",\n",
    "    \"fall_type\",\n",
    "    \"fall_year\",\n",
    "    \"class_id\",\n",
    "    \"location_id\"\n",
    "]].copy()\n",
    "\n",
    "# Verify\n",
    "print(\"Information on metrodf:\")\n",
    "metrodf.info()\n",
    "\n",
    "# Save the cleaned metrodf\n",
    "metrodf.to_csv(\"metrodf_cleaned.csv\", index=False)\n",
    "print(\"metrodf_cleaned.csv written successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed99521-cda2-4b73-beb8-dd60d5731b45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13b53c0-60c1-4255-acc7-1baaab86bf33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ea1e23-6bb9-45e0-a5df-3c3d0036890f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6004660-d850-4b90-881e-fbb5fb93f675",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
